\documentclass[11pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how tolay out the page. There's lots.
\usepackage{caption}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{pdflscape}
\usepackage[toc,page]{appendix}
\usepackage{float}
\usepackage{sidecap}
\usepackage{minted}
\usepackage{listings}
\usepackage{circuitikz}
\usepackage{hyperref}
\usepackage[style=chicago-authordate]{biblatex} %Imports biblatex package

\geometry{a4paper} % orletter or a5paper or ... etc %
\geometry{left=2.5cm, right=2.5cm, bottom=2.5cm, top = 2.5cm}
\setlength{\intextsep}{5pt} % Vertical space above & below [h] floats
\setlength{\textfloatsep}{5pt} % Vertical space below (above) [t] ([b]) floats
\setlength{\abovecaptionskip}{5pt}
\setlength{\belowcaptionskip}{5pt}

\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}


\author{CCN: 5654F}
\title[4F13 - Coursework \#2: Probabilistic Ranking]{\vspace{-.4in}4F13 - Coursework \#2: Probabilistic Ranking\vspace{-.2in}}
\date{Wednesday 15th November 2023} % delete this line to display the current date4340-


%%% BEGIN DOCUMENT
\begin{document}
\maketitle
\vspace{-.4in}
\section{Task A: Gibbs Sampling} \label{sec:a}
The code in \verb|gibbsrank.py| was completed by adding functionality to generate the sample skills given performance differences by adding line 1 of Listing \ref{lst:cw2a} to the first uncompleted loop over all the players (which negates the skill difference of lost games from those won). Lines 4 onwards of the same Listing were then added to build the \verb|iS| matrix, by adding $\pm1$ to each entry to match the form of the covariance matrix in the lecture notes\footnote{\label{fn:l_notes}CUED 4F13 Lecture notes \href{Uhttps://mlg.eng.cam.ac.uk/teaching/4f13/2324/RL}{(course site)}}.

\begin{listing}[h]
    \begin{minted}[linenos,breaklines,fontsize=\footnotesize]{python3}
        m[p] = sum(t[np.where(G[:, 0] == p)]) - sum(
                t[np.where(G[:, 1] == p)]
            )
        
        winner = G[g, 0]
        loser = G[g, 1]

        iS[winner, winner] += 1
        iS[loser, loser] += 1
        iS[winner, loser] -= 1
        iS[loser, winner] -= 1
    \end{minted}
    \caption{Task A code excerpts} \label{lst:cw2a}
\end{listing}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.62\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{../plots/Ta_sample_skills.png}
        \caption{Player Skills}
        \label{fig:ta_skills}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.37\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{../plots/Ta_acorr.png}
        \caption{Player Autocorrelations}
        \label{fig:ta_autocor}
    \end{subfigure}
    \caption{Task A Figures}
\end{figure} \label{fig:taska_all}

The Gibbs sampler was then run for 1100 iterations using the provided python notebook, and 4 random players skills were plotted across the first 200 iterations: Figure \ref{fig:ta_skills}. The data is noisy but is clearly not completely independent as each skill point stays fairly close to the previous point. The autocorrelation for each player for 25 steps is plotted in Figure \ref{fig:ta_autocor}, which shows that an autocorrelation time of at least 10 samples is needed for the autocorrelation over the skills to be 0 for all athletes. Plotting the average population mean and variance also shows that the burn-in time (time for the chain to converge to the distribution) is lower than the 10 samples needed for the samples to become independent. Therefore the first 10 samples need to be discarded.

% \begin{figure}[]
%     \centering
%     \includegraphics[width=0.99\textwidth]{../plots/Ta_sample_skills.png}
%     \caption{Player Skills}
%     % \label{fig:ta_skills}
% \end{figure}


\section{Task B: Message Passing} \label{sec:b}
Convergence occurs when the parameters \emph{converge} to a constant value (the true value according to the algorithm). For Gibbs Sampling this is when the the Markov chain converges to a stationary probability distribution (the joint skill distribution). For message passing/EP this occurs when a stable graph is reached, each iteration of messages passed does not affect the mean and precision of each vertex. In Section \ref{sec:a} we saw that the \verb|max|(burn-in time, autocorrelation time) was $\sim$10 samples. A new function was first added to the \verb|eprank.py| that returns the means and precisions at each iteration, which allowed graphs of the change in mean and precision from their final values to be produced as Figure \ref{fig:taskb_all}. These show a more rapid convergence than the Gibbs sampling method, with the player means and precisions each reaching their final values in only $\sim$6 iterations, a 40\% improvement over the method in Section \ref{sec:a}.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{../plots/change_in_mean_normal.png}
        \caption{Player Mean Convergence}
        \label{fig:tb_means}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{../plots/change_in_precision_normal.png}
        \caption{Player Precision Convergence}
        \label{fig:tb_precs}
    \end{subfigure}
    \caption{Task B Figures}
\end{figure} \label{fig:taskb_all}

\section{Task C: Player Ranking using EP}
Players $ i,j $ have skills $ w_i, w_j $. $ p(w_i > w_j) $ can be easily calculated using the definitions in the lecture notes\footnote{see footnote \ref{fn:l_notes}} as in Equation \ref{eqn:skilldiff}. Similarly, a player wins against another in this model if the skill difference plus a noise $ n \sim \mathcal{N}(0,1)$ is greater than zero for $i$, or less than zero for $j$. Therefore the probability of $i$ winning is given by Equation \ref{eqn:win}.

\begin{eqnarray}
    p(w_i > w_j) = p(w_j-w_i < 0) =\Phi \left(\frac{\mu_i - \mu_j}{\sqrt{\lambda_i^{-1} + \lambda_j^{-1}}}\right) \\ \label{eqn:skilldiff}
    p(w_i - w_j + n > 0) = \Phi \left(\frac{\mu_i - \mu_j}{\sqrt{\lambda_i^{-1} + \lambda_j^{-1} + 1}}\right) \label{eqn:win}
\end{eqnarray}

\begin{listing}[h]
    \begin{minted}[linenos,breaklines,fontsize=\footnotesize]{python3}
        prob_has_better_skill = 1.0 - norm.cdf(
            0, mean_differences, vars_sums**0.5
        )

        prob_win = 1.0 - norm.cdf(
            0, mean_differences, (vars_sums + 1.0) ** 0.5
        )
    \end{minted}
    \caption{Task C code excerpts} \label{lst:cw2c}
\end{listing}

Lines 1 and 4 of Listing \ref{lst:cw2c} are therefore used to calculate a matrix showing the probabilities of the top 4 ranked players (from the lecture notes' ranking\footnote{see footnote \ref{fn:l_notes}}) a. having a higher skill than the others, and b. their probability of winning if playing against the other players. These lines just demonstrate the equations from the previous paragraph, using the \verb|scipy.stats.norm.cdf| function for $\Phi$. The results are shown in Tables \ref{tab:skill_prob} and \ref{tab:win_prob} respectively.
\begin{table}
    \centering{}
    \begin{minipage}[t]{0.495\textwidth}
        \begin{center}
            \begin{tabular}{lcccc}\toprule
                ---         & D           & N           & F           & M           \\ \midrule
                Djokovic(D) & \num{0.500} & \num{0.940} & \num{0.909} & \num{0.985} \\
                Nadal(N)    & \num{0.060} & \num{0.500} & \num{0.427} & \num{0.767} \\
                Federer(F)  & \num{0.091} & \num{0.573} & \num{0.500} & \num{0.811} \\
                Murray(M)   & \num{0.015} & \num{0.233} & \num{0.189} & \num{0.500} \\  \bottomrule
            \end{tabular}%
            \par\end{center}
        \caption{p(Higher Skill)} \label{tab:skill_prob}
        %
    \end{minipage}%
    \begin{minipage}[t]{0.495\textwidth}%
        \begin{center}
            \begin{tabular}{lcccc} \toprule
                ---         & D           & N           & F           & M           \\ \midrule
                Djokovic(D) & \num{0.500} & \num{0.655} & \num{0.638} & \num{0.720} \\
                Nadal(N)    & \num{0.345} & \num{0.500} & \num{0.482} & \num{0.573} \\
                Federer(F)  & \num{0.362} & \num{0.518} & \num{0.500} & \num{0.591} \\
                Murray(M)   & \num{0.280} & \num{0.427} & \num{0.409} & \num{0.500} \\ \bottomrule
            \end{tabular}
            \par\end{center}
        \caption{p(Win)} \label{tab:win_prob}
        %
    \end{minipage}%
    \label{tab:taskc_tabs}
\end{table}

It can clearly be seen that a player with higher expected skill than another is always expected to win the match between them, and the converse. However, the probability of winning is much lower than the probability of having a higher skill due to the additional match variance term $n$ (the performance noise). This adds to the model the fact that a player can have an ``off match'', and aids in training by making the model less confident in attributing skill to the winning player of each match.


\section{Task D: Nadal and Federer Skill Comparison}
\subsection{Approximation of Marginal Skills by Gaussians}



\section{Task E}


% \renewcommand{\thepage}{A\arabic{page}} 
% \renewcommand{\thetable}{A\arabic{table}}  
% \renewcommand{\thefigure}{A\arabic{figure}}
% \renewcommand{\theequation}{A\arabic{equation}}
% \setcounter{figure}{0}
% % \setcounter{page}{0}
% \setcounter{equation}{0}


% \begin{appendices}


% \clearpage



% \begin{center}
%     % \centering
%     \includegraphics[width=0.3\textwidth]{pics/Screenshot 2023-02-07 at 21.45.49.png}
%   \captionof{figure}{FPGA CAD Workflow \parencite{labhandout}} \label{fig:workflow}
% %   \small\textsuperscript{Diagram taken from CUED 3B2 Lab Handout}
% \end{center}

% \clearpage
% \section{Initial Traffic Light Timer Code} \label{sec:initcode}
% \inputminted[linenos,breaklines]{vhdl}{../initialcode.vhdl}


% \end{appendices}
\end{document}




% \begin{Figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{pics/FPGA_cell_example.png}
%   \captionof{figure}{Example of a Logic Cell \parencite{fpgawiki}} \label{fig:logiccell}
% %   \small\textsuperscript{Diagram taken from CUED 3B2 Lab Handout}
% \end{Figure}